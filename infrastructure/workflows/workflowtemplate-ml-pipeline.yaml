apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ml-pipeline-template
  namespace: argo-workflows
spec:
  entrypoint: pipeline
  serviceAccountName: argo-workflow

  arguments:
    parameters:
      - name: experiment-name-mlflow
        value: "argo-wf-pipeline"

      # MinIO input
      - name: bucket-name
        value: "input-data"
      - name: filename
        value: "weatherAUS.csv"

      # local paths inside container
      - name: local-data-path
        value: "/tmp/data/weatherAUS.csv"

      # artifact dirs (persisted via Argo artifacts between steps)
      - name: eda-out-dir
        value: "artifacts/eda"
      - name: preprocess-out-dir
        value: "artifacts/preprocess"
      - name: model-out-dir
        value: "artifacts/model"
      - name: eval-out-dir
        value: "artifacts/eval"

      # preprocessing params
      - name: test-size
        value: "0.2"
      - name: random-state
        value: "42"

      # train params (MLflow)
      - name: mlflow-experiment
        value: "weatherAUS"
      - name: train-run-name
        value: "logreg_baseline"

  templates:

  # =========================================================
  # Pipeline definition
  # =========================================================
  - name: pipeline
    steps:
      - - name: fetch-data
          template: fetch-from-minio
          arguments:
            parameters:
              - name: bucket-name
                value: "{{workflow.parameters.bucket-name}}"
              - name: filename
                value: "{{workflow.parameters.filename}}"
              - name: local-data-path
                value: "{{workflow.parameters.local-data-path}}"

      - - name: eda
          template: run-script-with-output-dir
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
            parameters:
              - name: script
                value: exploratory_data_analysis.py
              - name: out-dir
                value: "{{workflow.parameters.eda-out-dir}}"
              - name: cli-args
                value: >-
                  --data-path {{workflow.parameters.local-data-path}}
                  --out-dir {{workflow.parameters.eda-out-dir}}

      - - name: preprocess
          template: run-script-with-output-dir
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
            parameters:
              - name: script
                value: preprocessing.py
              - name: out-dir
                value: "{{workflow.parameters.preprocess-out-dir}}"
              - name: cli-args
                value: >-
                  --data-path {{workflow.parameters.local-data-path}}
                  --out-dir {{workflow.parameters.preprocess-out-dir}}
                  --test-size {{workflow.parameters.test-size}}
                  --random-state {{workflow.parameters.random-state}}

      - - name: train
          template: run-script-with-output-dir
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
              # bring preprocess artifacts back into the expected directory
              - name: input-preprocess
                from: "{{steps.preprocess.outputs.artifacts.out}}"
                path: "{{workflow.parameters.preprocess-out-dir}}"
            parameters:
              - name: script
                value: train.py
              - name: out-dir
                value: "{{workflow.parameters.model-out-dir}}"
              - name: cli-args
                value: >-
                  --preprocess-dir {{workflow.parameters.preprocess-out-dir}}
                  --out-dir {{workflow.parameters.model-out-dir}}
                  --experiment {{workflow.parameters.mlflow-experiment}}
                  --run-name {{workflow.parameters.train-run-name}}

      - - name: evaluate
          template: run-script-with-output-dir
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
              - name: input-preprocess
                from: "{{steps.preprocess.outputs.artifacts.out}}"
                path: "{{workflow.parameters.preprocess-out-dir}}"
              - name: input-model
                from: "{{steps.train.outputs.artifacts.out}}"
                path: "{{workflow.parameters.model-out-dir}}"
            parameters:
              - name: script
                value: evaluation.py
              - name: out-dir
                value: "{{workflow.parameters.eval-out-dir}}"
              - name: cli-args
                value: >-
                  --preprocess-dir {{workflow.parameters.preprocess-out-dir}}
                  --model-path {{workflow.parameters.model-out-dir}}/model.joblib
                  --out-dir {{workflow.parameters.eval-out-dir}}

  # =========================================================
  # Fetch step (MinIO S3 -> local file) + output dataset artifact
  # =========================================================
  - name: fetch-from-minio
    inputs:
      parameters:
        - name: bucket-name
        - name: filename
        - name: local-data-path
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail
          mkdir -p "$(dirname {{inputs.parameters.local-data-path}})"
          python - << 'PY'
          import os
          import boto3

          bucket = os.environ.get("BUCKET_NAME")
          key    = os.environ.get("FILENAME")
          out    = os.environ.get("LOCAL_DATA_PATH")

          s3 = boto3.client(
              "s3",
              endpoint_url=os.environ.get("AWS_ENDPOINT_URL"),
              aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
              aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
              region_name=os.environ.get("AWS_DEFAULT_REGION", "us-east-1"),
          )
          s3.download_file(bucket, key, out)
          print(f"[OK] Downloaded s3://{bucket}/{key} -> {out}")
          PY
      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: BUCKET_NAME
          value: "{{inputs.parameters.bucket-name}}"
        - name: FILENAME
          value: "{{inputs.parameters.filename}}"
        - name: LOCAL_DATA_PATH
          value: "{{inputs.parameters.local-data-path}}"

        # keep endpoints/secrets exactly as before
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio
              key: root-user
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio
              key: root-password
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-admin
              key: postgres-user

        - name: AWS_ENDPOINT_URL
          value: http://minio.data-storage.svc.cluster.local:9000
        - name: AWS_DEFAULT_REGION
          value: us-east-1
        - name: AWS_S3_ADDRESSING_STYLE
          value: path

        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-admin
              key: postgres-password

    outputs:
      artifacts:
        - name: dataset
          path: "{{inputs.parameters.local-data-path}}"

  # =========================================================
  # Script runner that ALSO returns an output directory as artifact
  # =========================================================
  - name: run-script-with-output-dir
    inputs:
      parameters:
        - name: script
        - name: cli-args
        - name: out-dir
      artifacts:
        - name: dataset
          path: "{{workflow.parameters.local-data-path}}"
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail
          python {{inputs.parameters.script}} {{inputs.parameters.cli-args}}
          # Helpful listing for debugging (optional)
          echo "[INFO] Contents of {{inputs.parameters.out-dir}}:"
          ls -la "{{inputs.parameters.out-dir}}" || true
      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: minio
              key: root-user
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: minio
              key: root-password
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef:
              name: postgres-admin
              key: postgres-user
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef:
              name: postgres-admin
              key: postgres-password

    outputs:
      artifacts:
        - name: out
          path: "{{inputs.parameters.out-dir}}"
