apiVersion: argoproj.io/v1alpha1
kind: WorkflowTemplate
metadata:
  name: ml-pipeline-template
  namespace: argo-workflows
spec:
  entrypoint: pipeline
  serviceAccountName: argo-workflow

  arguments:
    parameters:
      # MinIO input
      - name: bucket-name
        value: "input-data"
      - name: filename
        value: "weatherAUS.csv"

      # local dataset path INSIDE container
      - name: local-data-path
        value: "/tmp/data/weatherAUS.csv"

      # dirs produced/consumed by scripts (persisted via Argo artifacts)
      - name: eda-out-dir
        value: "/app/artifacts/eda"
      - name: preprocess-out-dir
        value: "/app/artifacts/preprocess"
      - name: model-out-dir
        value: "/app/artifacts/model"
      - name: eval-out-dir
        value: "/app/artifacts/eval"

      # preprocessing params
      - name: test-size
        value: "0.2"
      - name: random-state
        value: "42"

      # train params (MLflow)
      - name: mlflow-experiment
        value: "weatherAUS"
      - name: train-run-name
        value: "logreg_baseline"

  templates:

  # =========================================================
  # Pipeline definition (DAG via steps)
  # =========================================================
  - name: pipeline
    steps:
      - - name: fetch-data
          template: fetch-from-minio
          arguments:
            parameters:
              - name: bucket-name
                value: "{{workflow.parameters.bucket-name}}"
              - name: filename
                value: "{{workflow.parameters.filename}}"
              - name: local-data-path
                value: "{{workflow.parameters.local-data-path}}"

      - - name: eda
          template: eda-step
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
            parameters:
              - name: data-path
                value: "{{workflow.parameters.local-data-path}}"
              - name: out-dir
                value: "{{workflow.parameters.eda-out-dir}}"

      - - name: preprocess
          template: preprocess-step
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
            parameters:
              - name: data-path
                value: "{{workflow.parameters.local-data-path}}"
              - name: out-dir
                value: "{{workflow.parameters.preprocess-out-dir}}"
              - name: test-size
                value: "{{workflow.parameters.test-size}}"
              - name: random-state
                value: "{{workflow.parameters.random-state}}"

      - - name: train
          template: train-step
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
              - name: preprocess_art
                from: "{{steps.preprocess.outputs.artifacts.preprocess_out}}"
            parameters:
              - name: preprocess-dir
                value: "{{workflow.parameters.preprocess-out-dir}}"
              - name: out-dir
                value: "{{workflow.parameters.model-out-dir}}"
              - name: experiment
                value: "{{workflow.parameters.mlflow-experiment}}"
              - name: run-name
                value: "{{workflow.parameters.train-run-name}}"

      - - name: evaluate
          template: eval-step
          arguments:
            artifacts:
              - name: dataset
                from: "{{steps.fetch-data.outputs.artifacts.dataset}}"
              - name: preprocess_art
                from: "{{steps.preprocess.outputs.artifacts.preprocess_out}}"
              - name: model_art
                from: "{{steps.train.outputs.artifacts.model_out}}"
            parameters:
              - name: preprocess-dir
                value: "{{workflow.parameters.preprocess-out-dir}}"
              - name: model-path
                value: "{{workflow.parameters.model-out-dir}}/model.joblib"
              - name: out-dir
                value: "{{workflow.parameters.eval-out-dir}}"

  # =========================================================
  # Fetch from MinIO (S3) -> writes local file -> outputs dataset artifact
  # =========================================================
  - name: fetch-from-minio
    inputs:
      parameters:
        - name: bucket-name
        - name: filename
        - name: local-data-path
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail
          mkdir -p "$(dirname {{inputs.parameters.local-data-path}})"
          python - << 'PY'
          import os, boto3

          bucket = os.environ["BUCKET_NAME"]
          key    = os.environ["FILENAME"]
          out    = os.environ["LOCAL_DATA_PATH"]

          s3 = boto3.client(
              "s3",
              endpoint_url=os.environ.get("AWS_ENDPOINT_URL"),
              aws_access_key_id=os.environ.get("AWS_ACCESS_KEY_ID"),
              aws_secret_access_key=os.environ.get("AWS_SECRET_ACCESS_KEY"),
              region_name=os.environ.get("AWS_DEFAULT_REGION", "us-east-1"),
          )
          s3.download_file(bucket, key, out)
          print(f"[OK] Downloaded s3://{bucket}/{key} -> {out}")
          PY
      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: BUCKET_NAME
          value: "{{inputs.parameters.bucket-name}}"
        - name: FILENAME
          value: "{{inputs.parameters.filename}}"
        - name: LOCAL_DATA_PATH
          value: "{{inputs.parameters.local-data-path}}"

        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef: { name: minio, key: root-user }
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio, key: root-password }

        - name: AWS_ENDPOINT_URL
          value: http://minio.data-storage.svc.cluster.local:9000
        - name: AWS_DEFAULT_REGION
          value: us-east-1
        - name: AWS_S3_ADDRESSING_STYLE
          value: path

        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-user }
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-password }

    outputs:
      artifacts:
        - name: dataset
          path: "{{inputs.parameters.local-data-path}}"

  # =========================================================
  # EDA step: consumes dataset artifact, outputs eda directory artifact
  # =========================================================
  - name: eda-step
    inputs:
      parameters:
        - name: data-path
        - name: out-dir
      artifacts:
        - name: dataset
          path: "{{workflow.parameters.local-data-path}}"
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail
          python exploratory_data_analysis.py \
            --data-path {{inputs.parameters.data-path}} \
            --out-dir {{inputs.parameters.out-dir}}
          ls -la {{inputs.parameters.out-dir}} || true
      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef: { name: minio, key: root-user }
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio, key: root-password }
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-user }
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-password }
    outputs:
      artifacts:
        - name: eda_out
          path: "{{inputs.parameters.out-dir}}"

  # =========================================================
  # Preprocess step: consumes dataset artifact, outputs preprocess directory artifact
  # =========================================================
  - name: preprocess-step
    inputs:
      parameters:
        - name: data-path
        - name: out-dir
        - name: test-size
        - name: random-state
      artifacts:
        - name: dataset
          path: "{{workflow.parameters.local-data-path}}"
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail
          python preprocessing.py \
            --data-path {{inputs.parameters.data-path}} \
            --out-dir {{inputs.parameters.out-dir}} \
            --test-size {{inputs.parameters.test-size}} \
            --random-state {{inputs.parameters.random-state}}
          ls -la {{inputs.parameters.out-dir}} || true
      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef: { name: minio, key: root-user }
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio, key: root-password }
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-user }
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-password }
    outputs:
      artifacts:
        - name: preprocess_out
          path: "{{inputs.parameters.out-dir}}"

  # =========================================================
  # Train step: consumes preprocess artifact (mounted to preprocess-out-dir), outputs model dir artifact
  # =========================================================
  - name: train-step
    inputs:
      parameters:
        - name: preprocess-dir
        - name: out-dir
        - name: experiment
        - name: run-name
      artifacts:
        - name: dataset
          path: "{{workflow.parameters.local-data-path}}"
        - name: preprocess_art
          path: "{{workflow.parameters.preprocess-out-dir}}"
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail

          # --- compat: some scripts expect relative "artifacts/..."
          mkdir -p /app/artifacts
          mkdir -p artifacts
          ln -sfn /app/artifacts/preprocess artifacts/preprocess

          # (optional debug)
          echo "[DEBUG] preprocess-dir={{inputs.parameters.preprocess-dir}}"
          ls -la /app/artifacts/preprocess || true
          ls -la artifacts/preprocess || true

          python train.py \
            --preprocess-dir {{inputs.parameters.preprocess-dir}} \
            --out-dir {{inputs.parameters.out-dir}} \
            --experiment {{inputs.parameters.experiment}} \
            --run-name {{inputs.parameters.run-name}}

          ls -la {{inputs.parameters.out-dir}} || true

      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef: { name: minio, key: root-user }
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio, key: root-password }
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-user }
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-password }
    outputs:
      artifacts:
        - name: model_out
          path: "{{inputs.parameters.out-dir}}"

  # =========================================================
  # Eval step: consumes preprocess + model artifacts, outputs eval dir artifact
  # =========================================================
  - name: eval-step
    inputs:
      parameters:
        - name: preprocess-dir
        - name: model-path
        - name: out-dir
      artifacts:
        - name: dataset
          path: "{{workflow.parameters.local-data-path}}"
        - name: preprocess_art
          path: "{{workflow.parameters.preprocess-out-dir}}"
        - name: model_art
          path: "{{workflow.parameters.model-out-dir}}"
    container:
      image: ghcr.io/jbderleuchtturm/ml-weather-pipeline:0.4
      command: ["bash", "-c"]
      args:
        - |
          set -euo pipefail

          # --- compat: scripts may expect relative "artifacts/..."
          mkdir -p /app/artifacts
          mkdir -p artifacts
          ln -sfn /app/artifacts/preprocess artifacts/preprocess
          ln -sfn /app/artifacts/model artifacts/model

          python evaluation.py \
            --preprocess-dir {{inputs.parameters.preprocess-dir}} \
            --model-path {{inputs.parameters.model-path}} \
            --out-dir {{inputs.parameters.out-dir}}

          ls -la {{inputs.parameters.out-dir}} || true

      envFrom:
        - configMapRef:
            name: mlops-runtime-config
      env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef: { name: minio, key: root-user }
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef: { name: minio, key: root-password }
        - name: POSTGRES_USER
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-user }
        - name: POSTGRES_PASSWORD
          valueFrom:
            secretKeyRef: { name: postgres-admin, key: postgres-password }
    outputs:
      artifacts:
        - name: eval_out
          path: "{{inputs.parameters.out-dir}}"
